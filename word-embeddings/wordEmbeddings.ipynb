{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings on Harry Potter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (Notebook by Itay Hazan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\etayh\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Use gensim implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads the entire Harry Potter series into a list, split by periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_harry_potter_books():\n",
    "    books = []\n",
    "    for i in range(7):\n",
    "        with open('HarryPotter/{}.txt'.format(i+1)) as f:\n",
    "            books += f.read().split('.')\n",
    "            \n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = get_harry_potter_books()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function, that does some basic pre-processing on the texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(books): \n",
    "    # TODO: lowercase\n",
    "    # TODO: remove all end-of-line characters\n",
    "    # TODO: remove all punctuation\n",
    "    # TODO: tokenize words (=split by whitespaces)\n",
    "    lst = []\n",
    "    \n",
    "    #return a list of lists: element i of the outer list is a list of word in the i'th book\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = pre_processing(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the first ten sentences (after pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'boy', 'who', 'lived', 'mr']\n",
      "['and', 'mrs']\n",
      "['dursley', 'of', 'number', 'four', 'privet', 'drive', 'were', 'proud', 'to', 'say', 'that', 'they', 'were', 'perfectly', 'normal', 'thank', 'you', 'very', 'much']\n",
      "['they', 'were', 'the', 'last', 'people', 'you', 'expect', 'to', 'be', 'involved', 'in', 'anything', 'strange', 'or', 'mysterious', 'because', 'they', 'just', 'didn', 'hold', 'with', 'such', 'nonsense']\n",
      "['mr']\n",
      "['dursley', 'was', 'the', 'director', 'of', 'firm', 'called', 'grunnings', 'which', 'made', 'drills']\n",
      "['he', 'was', 'big', 'beefy', 'man', 'with', 'hardly', 'any', 'neck', 'although', 'he', 'did', 'have', 'very', 'large', 'mustache']\n",
      "['mrs']\n",
      "['dursley', 'was', 'thin', 'and', 'blonde', 'and', 'had', 'nearly', 'twice', 'the', 'usual', 'amount', 'of', 'neck', 'which', 'came', 'in', 'very', 'useful', 'as', 'she', 'spent', 'so', 'much', 'of', 'her', 'time', 'craning', 'over', 'garden', 'fences', 'spying', 'on', 'the', 'neighbors']\n",
      "['the', 'dursleys', 'had', 'small', 'son', 'called', 'dudley', 'and', 'in', 'their', 'opinion', 'there', 'was', 'no', 'finer', 'boy', 'anywhere']\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(books[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86672"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(books)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize a Word2Vec model from gensim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8025370, 10597000)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(books, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(books, total_examples=len(books),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('grawp', 0.600788950920105),\n",
       " ('yeh', 0.5458800196647644),\n",
       " ('fang', 0.5186895132064819),\n",
       " ('boarhound', 0.49009260535240173),\n",
       " ('anythin', 0.4781782925128937),\n",
       " ('bane', 0.4736838936805725),\n",
       " ('wha', 0.47167322039604187),\n",
       " ('buckbeak', 0.4709952473640442),\n",
       " ('em', 0.46213701367378235),\n",
       " ('hippogriff', 0.45905590057373047)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv = lambda x: model.wv.word_vec(x)\n",
    "\n",
    "# w1 = \"lord\"\n",
    "# model.wv.most_similar(positive=[wv(\"harry\")-wv(\"magic\")+wv(\"muggle\")])\n",
    "\n",
    "model.wv.most_similar(positive=\"hagrid\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Implementing CBOW with Negative Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.1: Setting things up "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, get a list of all unique words in the dataset, and sort them alphabetically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = \n",
    "\n",
    "num_words = len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create an inverted index of the words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "# TODO: vocab[word] = index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the number of occurances of each word in our dataset (a histogram):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = []\n",
    "# TODO: complete hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given the histogram $h$, write a function that returns a probability distribution over the words in the histgram such that a more popular word will have a higher probability of being chosen:\n",
    "$$ \\Pr [\\text{sampling $i$'th word}] = \\frac{hist[i]}{\\sum_i hist[i]} $$\n",
    "\n",
    "Remark: it is customary to take the elemets in the right-hand side of the equality to some power smaller than 1, e.g.:\n",
    "$$ \\Pr [\\text{sampling $i$'th word}] = \\frac{hist[i] ^{3/4}}{\\sum_i hist[i]^{3/4}} $$\n",
    "You may use this in your code as well (ampirically gives better performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Construct the train set\n",
    "We define the window size and the dimension of the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 10\n",
    "neg_sample_size = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our train set will consist of labeled pairs: ` (x=(context, center), y=0/1`)`: \n",
    "\n",
    "To create the train set:\n",
    "\n",
    " 1. For every `window=(context, center)` of the input\n",
    "   1. Add the pair `(x=(context, center), y=1)` to the dataset.\n",
    "   1. Sample `neg_sample_size` words, `w_1, ..., w_k`, from the distribution we computed in step 2.1, and add the all the pairs `(x=(context, w_i), y=0)` to the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Construct the neural net\n",
    "We are going to create the following network architecture for negative sampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Negative Sampling Architecture](neg_sampling.png \"Negative Sampling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 150 # dimension of the embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.4: Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that, given a word, returns the 10 most similar words to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(word):\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play with it :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
